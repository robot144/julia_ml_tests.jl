{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Convolutional Network with GeometrixFlux.jl\n",
    "\n",
    "This example is adapted from examples/gcm_with_Static_graph.jl in GeometrixFlux.jl. It is the first example that I try to run with this package.\n",
    "\n",
    "## Application to CORA\n",
    "\n",
    "The CORA dataset is a graph with papers (nodes=2708) and the references between them (edges). For each node there is a vector of length=1433 that \n",
    "gives the presence of words. For example for node 1 the words 20  82  147  316  775  878  1195  1248  1275 are present. The task of the network is to classify each node into one of 7 classes, using the presence of the words and the reference links in the graph. The application seems to be taken from\n",
    "[this paper](https://arxiv.org/pdf/1609.02907.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages - Needed only once   \n",
    "using Pkg\n",
    "Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA\n",
    "using Flux\n",
    "using Flux: onehotbatch, onecold\n",
    "using Flux.Losses: logitcrossentropy\n",
    "using Flux.Data: DataLoader\n",
    "using GeometricFlux\n",
    "using GeometricFlux.Datasets\n",
    "using Graphs\n",
    "using GraphSignals\n",
    "using Parameters: @with_kw\n",
    "using ProgressMeter: Progress, next!\n",
    "using Statistics\n",
    "using Random\n",
    "using Plots, GraphRecipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some paramaters\n",
    "\n",
    "η = 0.01                # learning rate\n",
    "λ = 5f-4                # regularization paramater\n",
    "batch_size = 32         # batch size\n",
    "epochs = 2              # number of epochs (was 200)\n",
    "seed = 0                # random seed\n",
    "cuda = true             # use GPU\n",
    "input_dim = 1433        # input dimension\n",
    "hidden_dim = 16         # hidden dimension\n",
    "target_dim = 7          # target dimension\n",
    "dataset = Cora          # dataset to train on\n",
    "\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed for reproducibility\n",
    "seed > 0 && Random.seed!(seed)\n",
    "\n",
    "# GPU config\n",
    "if cuda && CUDA.has_cuda()\n",
    "    device = gpu\n",
    "    CUDA.allowscalar(false)\n",
    "    @info \"Training on GPU\"\n",
    "else\n",
    "    device = cpu\n",
    "    @info \"Training on CPU\"\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dsets=dataset()\n",
    "# meta=dsets.metadata\n",
    "# @show meta\n",
    "# graph1=dsets.graphs[1]\n",
    "# @show graph1\n",
    "\n",
    "# features = graph1.node_data.features\n",
    "# @show findall(x->(x>0.0),features[:,1])'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "function load_data(dataset, batch_size, train_repeats=512, test_repeats=32)\n",
    "    s, t = dataset[1].edge_index\n",
    "    g = Graphs.Graph(dataset[1].num_nodes)\n",
    "    for (i, j) in zip(s, t)\n",
    "        Graphs.add_edge!(g, i, j)\n",
    "    end\n",
    "\n",
    "    data = dataset[1].node_data\n",
    "    X, y = data.features, onehotbatch(data.targets, 1:7)\n",
    "    train_idx, test_idx = data.train_mask, data.val_mask\n",
    "\n",
    "    #MVL It seems that copies of the entire dataset are made here\n",
    "    #MVL This seems a waste of memory. Is this done to match the \n",
    "    #MVL normal minibatch cycle? Or do they want to create a larger dataser\n",
    "    #MVL for performance testing? The nodes that are actually used are\n",
    "    #MVL determined by the train_idx and test_idx masks.\n",
    "    #MVL This dasaset could be reduced to a fractions of its size for training\n",
    "    #MVL and testing. This would reduce the memory requirements and speed up\n",
    "    #MVL the training and testing.\n",
    "    # (train_X, train_y) dim: (num_features, target_dim) × 2708 × train_repeats\n",
    "    train_X, train_y = repeat(X, outer=(1,1,train_repeats)), repeat(y, outer=(1,1,train_repeats))\n",
    "    # (test_X, test_y) dim: (num_features, target_dim) × 2708 × test_repeats\n",
    "    test_X, test_y = repeat(X, outer=(1,1,test_repeats)), repeat(y, outer=(1,1,test_repeats))\n",
    "\n",
    "    fg = FeaturedGraph(g)\n",
    "    train_loader = DataLoader((train_X, train_y), batchsize=batch_size, shuffle=true)\n",
    "    test_loader = DataLoader((test_X, test_y), batchsize=batch_size, shuffle=true)\n",
    "    return train_loader, test_loader, fg, train_idx, test_idx\n",
    "end\n",
    "\n",
    "# load Cora from Planetoid dataset\n",
    "train_loader, test_loader, fg, train_idx, test_idx = load_data(dataset(), batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at data\n",
    "\n",
    "@show fg #Undirected graph with (#V=2708, #E=5278)\n",
    "@show train_loader\n",
    "\n",
    "(train_x, train_y) = first(train_loader)\n",
    "@show size(train_x) #(1433, 2708, 32) = (input_dim, #V, batch_size)\n",
    "@show size(train_y) #(7, 2708, 32) = (target_dim, #V, batch_size)\n",
    "(test_x, test_y) = first(test_loader)\n",
    "@show size(test_x) #(1433, 2708, 32) = (input_dim, #V, batch_size)  \n",
    "@show size(test_y) #(7, 2708, 32) = (target_dim, #V, batch_size)\n",
    "\n",
    "@show count(train_idx.>0) #140\n",
    "@show count(test_idx.>0) #500\n",
    "\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss: cross entropy with first layer L2 regularization \n",
    "l2norm(x) = sum(abs2, x)\n",
    "\n",
    "function model_loss(model, λ, X, y, idx)\n",
    "    loss = logitcrossentropy(model(X)[:,idx,:], y[:,idx,:])\n",
    "    loss += λ*sum(l2norm, Flux.params(model[1]))\n",
    "    return loss\n",
    "end\n",
    "\n",
    "accuracy(model, X::AbstractArray, y::AbstractArray, idx) =\n",
    "    mean(onecold(softmax(cpu(model(X))[:,idx,:])) .== onecold(cpu(y)[:,idx,:]))\n",
    "\n",
    "accuracy(model, loader::DataLoader, device, idx) =\n",
    "    mean(accuracy(model, X |> device, y |> device, idx) for (X, y) in loader)\n",
    "\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "model = Chain(\n",
    "    WithGraph(fg, GCNConv(input_dim=>hidden_dim, relu)),\n",
    "    Dropout(0.5),\n",
    "    WithGraph(fg, GCNConv(hidden_dim=>target_dim)),\n",
    ") |> device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adam optimizer\n",
    "opt = Adam(η)\n",
    "\n",
    "# parameters\n",
    "ps = Flux.params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop for Training\n",
    "\n",
    "# training\n",
    "train_steps = 0\n",
    "@info \"Start Training, total $(epochs) epochs\"\n",
    "for epoch = 1:epochs\n",
    "    @info \"Epoch $(epoch)\"\n",
    "    progress = Progress(length(train_loader))\n",
    "\n",
    "    for (X, y) in train_loader\n",
    "        X, y, device_idx = X |> device, y |> device, train_idx |> device\n",
    "        loss, back = Flux.pullback(() -> model_loss(model, λ, X, y, device_idx), ps)\n",
    "        train_acc = accuracy(model, train_loader, device, train_idx)\n",
    "        test_acc = accuracy(model, test_loader, device, test_idx)\n",
    "        grad = back(1f0)\n",
    "        Flux.Optimise.update!(opt, ps, grad)\n",
    "\n",
    "        # progress meter\n",
    "        next!(progress; showvalues=[\n",
    "            (:loss, loss),\n",
    "            (:train_accuracy, train_acc),\n",
    "            (:test_accuracy, test_acc)\n",
    "        ])\n",
    "\n",
    "        train_steps += 1\n",
    "    end\n",
    "end\n",
    "\n",
    "#return model, args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
