{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bdb6a12",
   "metadata": {},
   "source": [
    "# Lux Basics\n",
    "\n",
    "Follows: [https://lux.csail.mit.edu/stable/tutorials/beginner/1_Basics](https://lux.csail.mit.edu/stable/tutorials/beginner/1_Basics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Lux\n",
    "using Random\n",
    "using Plots\n",
    "using CUDA\n",
    "using LuxCUDA\n",
    "using ComponentArrays\n",
    "using ForwardDiff\n",
    "using Zygote # autodiff backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac4d8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0.6293451231426089, 0.4503389405961936, 0.47740714343281776, 0.7031298490032014, 0.6733461456394962]\n",
      "y = [0.4056994708920292, 0.06854582438651502, 0.8621408571954849, 0.08597086585842195, 0.6616126907308237]\n",
      "x3 = [0.6293451231426089, 0.4503389405961936, 0.47740714343281776, 0.7031298490032014, 0.6733461456394962]\n",
      "x4 = [0.6293451231426089, 0.4503389405961936, 0.47740714343281776, 0.7031298490032014, 0.6733461456394962]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " 0.6293451231426089\n",
       " 0.4503389405961936\n",
       " 0.47740714343281776\n",
       " 0.7031298490032014\n",
       " 0.6733461456394962"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng1 = Random.default_rng()\n",
    "Random.seed!(rng, 42) # Set the random seed for reproducibility\n",
    "rng2 = Xoshiro(0)\n",
    "# generate random data using rng\n",
    "x = rand(rng1, 5)\n",
    "@show x\n",
    "y = rand(rng2, 5) # different random number generator gives different values\n",
    "@show y\n",
    "\n",
    "# replication of the random number generator to control the random seed\n",
    "rng3 = Xoshiro(42)\n",
    "rng4 = Lux.replicate(rng3)\n",
    "x3 = rand(rng3, 5)\n",
    "@show x3\n",
    "x4 = rand(rng4, 5) # same random number generator gives same values        \n",
    "@show x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "489692a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: LuxCUDA is loaded but the CUDA GPU is not functional.\n",
      "└ @ LuxCUDA /home/verlaan/.julia/packages/LuxCUDA/rqXwj/src/LuxCUDA.jl:20\n"
     ]
    }
   ],
   "source": [
    "# Use CUDA\n",
    "using LuxCUDA\n",
    "\n",
    "if LuxCUDA.functional()\n",
    "    x_cu = cu(rand(5, 3))\n",
    "    @show x_cu\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ddc2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Gradient: Float32[-0.37426844, 1.1695118, 0.25684848, 0.20092508]\n",
      "Computed Gradient via Reverse Mode AD (Zygote): Float32[-0.37426844, 1.1695118, 0.25684848, 0.20092508]\n",
      "Computed Gradient via Forward Mode AD (ForwardDiff): Float32[-0.37426844, 1.1695118, 0.25684848, 0.20092508]\n"
     ]
    }
   ],
   "source": [
    "# autodiff\n",
    "f(x) = x' * x / 2\n",
    "∇f(x) = x  # analytical gradient\n",
    "v = randn(rng, Float32, 4)\n",
    "\n",
    "# gradient\n",
    "println(\"Actual Gradient: \", ∇f(v))\n",
    "println(\"Computed Gradient via Reverse Mode AD (Zygote): \", only(Zygote.gradient(f, v))) #only unwraps the result (grad,)\n",
    "println(\"Computed Gradient via Forward Mode AD (ForwardDiff): \", ForwardDiff.gradient(f, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348250c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vextor x: Float32[-0.95902896, 0.9548453, 0.3785684, -0.25736085, -1.1705533]\n",
      "JVP: Float32[-0.95902896, 0.9548453, 0.3785684, -0.25736085, -1.1705533]\n",
      "VJP: Float32[-0.95902896, 0.9548453, 0.3785684, -0.25736085, -1.1705533]\n"
     ]
    }
   ],
   "source": [
    "# Slightly more advanced AD\n",
    "g(x) = x .* x ./ 2 # Jacobian is diagonal x\n",
    "x = randn(rng, Float32, 5)\n",
    "w = ones(Float32, 5)\n",
    "\n",
    "println(\"Vextor x: \", x)\n",
    "jvp = jacobian_vector_product(g, AutoForwardDiff(), x, w)\n",
    "println(\"JVP: \", jvp)\n",
    "vjp = vector_jacobian_product(g, AutoZygote(), x, w)\n",
    "println(\"VJP: \", vjp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba4042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ps, typeof(ps), st) = ((weight = Float32[-0.14353429 0.21060266 -0.44558594 0.10690627 -0.42703035 0.5472681 0.35721895 0.25983742 -0.030790705 -0.25331742; -0.3557504 0.15986149 0.41125566 -0.00062740635 -0.07626999 -0.38717833 0.18174236 0.39531672 -0.48462322 -0.42186803; 0.00021083308 0.021321498 -0.44888008 -0.11481372 0.1573712 0.33681953 -0.44941097 0.27972776 0.14055912 -0.35071373; 0.42093596 -0.4932969 0.2911111 0.3800604 0.36481598 0.10335287 0.05219121 -0.26366317 -0.26813912 0.392228; -0.5190183 -0.48312044 -0.15008768 0.34194311 -0.15554538 -0.45916212 0.08353097 0.032506883 -0.010100399 -0.022358427], bias = Float32[0.117224954, -0.29447955, 0.11671246, 0.06829139, 0.08250598]), @NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}, NamedTuple())\n",
      "(ps, typeof(ps)) = ((weight = Float32[-0.14353429 0.21060266 -0.44558594 0.10690627 -0.42703035 0.5472681 0.35721895 0.25983742 -0.030790705 -0.25331742; -0.3557504 0.15986149 0.41125566 -0.00062740635 -0.07626999 -0.38717833 0.18174236 0.39531672 -0.48462322 -0.42186803; 0.00021083308 0.021321498 -0.44888008 -0.11481372 0.1573712 0.33681953 -0.44941097 0.27972776 0.14055912 -0.35071373; 0.42093596 -0.4932969 0.2911111 0.3800604 0.36481598 0.10335287 0.05219121 -0.26366317 -0.26813912 0.392228; -0.5190183 -0.48312044 -0.15008768 0.34194311 -0.15554538 -0.45916212 0.08353097 0.032506883 -0.010100399 -0.022358427], bias = Float32[0.117224954, -0.29447955, 0.11671246, 0.06829139, 0.08250598]), ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(weight = ViewAxis(1:50, ShapedAxis((5, 10))), bias = ViewAxis(51:55, Shaped1DAxis((5,))))}}})\n",
      "x shape: (10, 20); y shape: (5, 20)\n",
      "Loss Value with true parameters: 8.312343e-5\n",
      "Loss Value with initial parameters: 14.46174\n",
      "Loss Value after      1 iterations: 14.46173954\n",
      "Loss Value after   1001 iterations: 0.11734114\n",
      "Loss Value after   2001 iterations: 0.01700308\n",
      "Loss Value after   3001 iterations: 0.00369752\n",
      "Loss Value after   4001 iterations: 0.00094456\n",
      "Loss Value after   5001 iterations: 0.00027413\n",
      "Loss Value after   6001 iterations: 0.00010023\n",
      "Loss Value after   7001 iterations: 0.00005402\n",
      "Loss Value after   8001 iterations: 0.00004163\n",
      "Loss Value after   9001 iterations: 0.00003829\n",
      "Loss Value after  10000 iterations: 0.00003739\n",
      "Loss Value after training: 3.7386293e-5\n"
     ]
    }
   ],
   "source": [
    "# Liear regression example with Lux\n",
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "model = Dense(x_dim => y_dim)\n",
    "ps, st = Lux.setup(rng, model)\n",
    "@show ps, typeof(ps), st # named tuple of parameters and state\n",
    "ps = ComponentArray(ps)\n",
    "@show ps, typeof(ps)\n",
    "\n",
    "# Generate the data with a known linear relationship and add some noise\n",
    "W = randn(rng, Float32, y_dim, x_dim)\n",
    "b = randn(rng, Float32, y_dim)\n",
    "x_samples = randn(rng, Float32, x_dim, n_samples)\n",
    "y_samples = W * x_samples .+ b .+ 0.01f0 .* randn(rng, Float32, y_dim, n_samples)\n",
    "println(\"x shape: \", size(x_samples), \"; y shape: \", size(y_samples))\n",
    "\n",
    "using Optimisers, Printf\n",
    "\n",
    "lossfn = MSELoss()\n",
    "\n",
    "println(\"Loss Value with true parameters: \", lossfn(W * x_samples .+ b, y_samples))\n",
    "\n",
    "# cost for initial parameters\n",
    "y_model = first(model(x_samples, ps, st))\n",
    "println(\"Loss Value with initial parameters: \", lossfn(y_model, y_samples))\n",
    "\n",
    "# Function to train the model\n",
    "function train_model!(model, ps, st, opt, nepochs::Int)\n",
    "    tstate = Training.TrainState(model, ps, st, opt)\n",
    "    for i in 1:nepochs\n",
    "        grads, loss, _, tstate = Training.single_train_step!(\n",
    "            AutoZygote(), lossfn, (x_samples, y_samples), tstate\n",
    "        )\n",
    "        if i % 1000 == 1 || i == nepochs\n",
    "            @printf \"Loss Value after %6d iterations: %.8f\\n\" i loss\n",
    "        end\n",
    "    end\n",
    "    return tstate.model, tstate.parameters, tstate.states\n",
    "end\n",
    "\n",
    "model, ps, st = train_model!(model, ps, st, Descent(0.01f0), 10000)\n",
    "\n",
    "println(\"Loss Value after training: \", lossfn(first(model(x_samples, ps, st)), y_samples)) # lower than for true parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fea8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedTuple()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fec35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
