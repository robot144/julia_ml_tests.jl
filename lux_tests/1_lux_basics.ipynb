{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bdb6a12",
   "metadata": {},
   "source": [
    "# Lux Basics\n",
    "\n",
    "Follows: [https://lux.csail.mit.edu/stable/tutorials/beginner/1_Basics](https://lux.csail.mit.edu/stable/tutorials/beginner/1_Basics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Lux\n",
    "using Random\n",
    "using Plots\n",
    "using CUDA\n",
    "using LuxCUDA\n",
    "using ComponentArrays\n",
    "using ForwardDiff\n",
    "using Zygote # autodiff backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac4d8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0.6293451231426089, 0.4503389405961936, 0.47740714343281776, 0.7031298490032014, 0.6733461456394962]\n",
      "y = [0.4056994708920292, 0.06854582438651502, 0.8621408571954849, 0.08597086585842195, 0.6616126907308237]\n",
      "x3 = [0.6293451231426089, 0.4503389405961936, 0.47740714343281776, 0.7031298490032014, 0.6733461456394962]\n",
      "x4 = [0.6293451231426089, 0.4503389405961936, 0.47740714343281776, 0.7031298490032014, 0.6733461456394962]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element Vector{Float64}:\n",
       " 0.6293451231426089\n",
       " 0.4503389405961936\n",
       " 0.47740714343281776\n",
       " 0.7031298490032014\n",
       " 0.6733461456394962"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rng1 = Random.default_rng()\n",
    "Random.seed!(rng, 42) # Set the random seed for reproducibility\n",
    "rng2 = Xoshiro(0)\n",
    "# generate random data using rng\n",
    "x = rand(rng1, 5)\n",
    "@show x\n",
    "y = rand(rng2, 5) # different random number generator gives different values\n",
    "@show y\n",
    "\n",
    "# replication of the random number generator to control the random seed\n",
    "rng3 = Xoshiro(42)\n",
    "rng4 = Lux.replicate(rng3)\n",
    "x3 = rand(rng3, 5)\n",
    "@show x3\n",
    "x4 = rand(rng4, 5) # same random number generator gives same values        \n",
    "@show x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "489692a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: LuxCUDA is loaded but the CUDA GPU is not functional.\n",
      "└ @ LuxCUDA /home/verlaan/.julia/packages/LuxCUDA/rqXwj/src/LuxCUDA.jl:20\n"
     ]
    }
   ],
   "source": [
    "# Use CUDA\n",
    "using LuxCUDA\n",
    "\n",
    "if LuxCUDA.functional()\n",
    "    x_cu = cu(rand(5, 3))\n",
    "    @show x_cu\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ddc2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Gradient: Float32[-0.37426844, 1.1695118, 0.25684848, 0.20092508]\n",
      "Computed Gradient via Reverse Mode AD (Zygote): Float32[-0.37426844, 1.1695118, 0.25684848, 0.20092508]\n",
      "Computed Gradient via Forward Mode AD (ForwardDiff): Float32[-0.37426844, 1.1695118, 0.25684848, 0.20092508]\n"
     ]
    }
   ],
   "source": [
    "# autodiff\n",
    "f(x) = x' * x / 2\n",
    "∇f(x) = x  # analytical gradient\n",
    "v = randn(rng, Float32, 4)\n",
    "\n",
    "# gradient\n",
    "println(\"Actual Gradient: \", ∇f(v))\n",
    "println(\"Computed Gradient via Reverse Mode AD (Zygote): \", only(Zygote.gradient(f, v))) #only unwraps the result (grad,)\n",
    "println(\"Computed Gradient via Forward Mode AD (ForwardDiff): \", ForwardDiff.gradient(f, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348250c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vextor x: Float32[-0.95902896, 0.9548453, 0.3785684, -0.25736085, -1.1705533]\n",
      "JVP: Float32[-0.95902896, 0.9548453, 0.3785684, -0.25736085, -1.1705533]\n",
      "VJP: Float32[-0.95902896, 0.9548453, 0.3785684, -0.25736085, -1.1705533]\n"
     ]
    }
   ],
   "source": [
    "# Slightly more advanced AD\n",
    "g(x) = x .* x ./ 2 # Jacobian is diagonal x\n",
    "x = randn(rng, Float32, 5)\n",
    "w = ones(Float32, 5)\n",
    "\n",
    "println(\"Vextor x: \", x)\n",
    "jvp = jacobian_vector_product(g, AutoForwardDiff(), x, w)\n",
    "println(\"JVP: \", jvp)\n",
    "vjp = vector_jacobian_product(g, AutoZygote(), x, w)\n",
    "println(\"VJP: \", vjp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba4042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ps, typeof(ps), st) = ((weight = Float32[0.1233335 0.42378408 0.062000398 -0.19484083 0.38523102 0.10262223 0.02096813 -0.42997092 0.3695187 -0.15907203; 0.42344913 0.40849015 -0.2285029 -0.50546885 0.4133596 0.09501148 0.20080848 -0.35760397 -0.023468288 -0.24797209; 0.34748197 0.5052108 0.038779575 -0.15346937 -0.19013004 -0.4029889 -0.16063868 -0.3044993 -0.18839604 0.13307981; 0.39337918 -0.38666537 0.11857732 0.21355523 0.43812287 -0.018205948 -0.4446723 0.25038174 0.26834545 0.47187915; -0.053774122 0.08671521 0.5185416 0.050246894 -0.31792536 0.124457 -0.32329604 0.44332144 -0.41103375 0.3870172], bias = Float32[0.22861098, -0.16324113, -0.080415174, 0.15626132, -0.17037798]), @NamedTuple{weight::Matrix{Float32}, bias::Vector{Float32}}, NamedTuple())\n",
      "(ps, typeof(ps)) = ((weight = Float32[0.1233335 0.42378408 0.062000398 -0.19484083 0.38523102 0.10262223 0.02096813 -0.42997092 0.3695187 -0.15907203; 0.42344913 0.40849015 -0.2285029 -0.50546885 0.4133596 0.09501148 0.20080848 -0.35760397 -0.023468288 -0.24797209; 0.34748197 0.5052108 0.038779575 -0.15346937 -0.19013004 -0.4029889 -0.16063868 -0.3044993 -0.18839604 0.13307981; 0.39337918 -0.38666537 0.11857732 0.21355523 0.43812287 -0.018205948 -0.4446723 0.25038174 0.26834545 0.47187915; -0.053774122 0.08671521 0.5185416 0.050246894 -0.31792536 0.124457 -0.32329604 0.44332144 -0.41103375 0.3870172], bias = Float32[0.22861098, -0.16324113, -0.080415174, 0.15626132, -0.17037798]), ComponentVector{Float32, Vector{Float32}, Tuple{Axis{(weight = ViewAxis(1:50, ShapedAxis((5, 10))), bias = ViewAxis(51:55, Shaped1DAxis((5,))))}}})\n",
      "x shape: (10, 20); y shape: (5, 20)\n",
      "Loss Value with true parameters: 8.748544e-5\n",
      "Loss Value with initial parameters: 9.909329\n",
      "Loss Value after      1 iterations: 9.90932941\n",
      "Loss Value after   1001 iterations: 0.10175665\n",
      "Loss Value after   2001 iterations: 0.02186811\n",
      "Loss Value after   3001 iterations: 0.00621590\n",
      "Loss Value after   4001 iterations: 0.00208967\n",
      "Loss Value after   5001 iterations: 0.00078735\n",
      "Loss Value after   6001 iterations: 0.00032496\n",
      "Loss Value after   7001 iterations: 0.00014969\n",
      "Loss Value after   8001 iterations: 0.00008109\n",
      "Loss Value after   9001 iterations: 0.00005385\n",
      "Loss Value after  10000 iterations: 0.00004297\n",
      "Loss Value after training: 4.2964966e-5\n"
     ]
    }
   ],
   "source": [
    "# Liear regression example with Lux\n",
    "n_samples = 20\n",
    "x_dim = 10\n",
    "y_dim = 5\n",
    "model = Dense(x_dim => y_dim)\n",
    "ps, st = Lux.setup(rng, model)\n",
    "@show ps, typeof(ps), st # named tuple of parameters and state\n",
    "ps = ComponentArray(ps)\n",
    "@show ps, typeof(ps)\n",
    "\n",
    "# Generate the data with a known linear relationship and add some noise\n",
    "W = randn(rng, Float32, y_dim, x_dim)\n",
    "b = randn(rng, Float32, y_dim)\n",
    "x_samples = randn(rng, Float32, x_dim, n_samples)\n",
    "y_samples = W * x_samples .+ b .+ 0.01f0 .* randn(rng, Float32, y_dim, n_samples)\n",
    "println(\"x shape: \", size(x_samples), \"; y shape: \", size(y_samples))\n",
    "\n",
    "using Optimisers, Printf\n",
    "\n",
    "lossfn = MSELoss()\n",
    "\n",
    "println(\"Loss Value with true parameters: \", lossfn(W * x_samples .+ b, y_samples))\n",
    "\n",
    "# cost for initial parameters\n",
    "y_model = first(model(x_samples, ps, st))\n",
    "println(\"Loss Value with initial parameters: \", lossfn(y_model, y_samples))\n",
    "\n",
    "function train_model!(model, ps, st, opt, nepochs::Int)\n",
    "    tstate = Training.TrainState(model, ps, st, opt)\n",
    "    for i in 1:nepochs\n",
    "        grads, loss, _, tstate = Training.single_train_step!(\n",
    "            AutoZygote(), lossfn, (x_samples, y_samples), tstate\n",
    "        )\n",
    "        if i % 1000 == 1 || i == nepochs\n",
    "            @printf \"Loss Value after %6d iterations: %.8f\\n\" i loss\n",
    "        end\n",
    "    end\n",
    "    return tstate.model, tstate.parameters, tstate.states\n",
    "end\n",
    "\n",
    "model, ps, st = train_model!(model, ps, st, Descent(0.01f0), 10000)\n",
    "\n",
    "println(\"Loss Value after training: \", lossfn(first(model(x_samples, ps, st)), y_samples)) # lower than for true parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fea8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NamedTuple()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595fec35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
