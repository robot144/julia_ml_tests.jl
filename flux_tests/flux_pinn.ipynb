{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics Informed Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "using Flux\n",
    "using Plots\n",
    "using Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching loss()\n\nClosest candidates are:\n  loss(!Matched::Any)\n   @ Main ~/src_nobackup/julia_ml_tests.jl.git/flux_pinn.ipynb:22\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching loss()\n",
      "\n",
      "Closest candidates are:\n",
      "  loss(!Matched::Any)\n",
      "   @ Main ~/src_nobackup/julia_ml_tests.jl.git/flux_pinn.ipynb:22\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      " [1] macro expansion\n",
      "   @ ~/.julia/packages/Zygote/JeHtr/src/compiler/interface2.jl:101 [inlined]\n",
      " [2] _pullback(::Zygote.Context{true}, ::typeof(loss))\n",
      "   @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface2.jl:101\n",
      " [3] pullback(f::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "   @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface.jl:384\n",
      " [4] withgradient(f::Function, args::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "   @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface.jl:132\n",
      " [5] top-level scope\n",
      "   @ ~/src_nobackup/julia_ml_tests.jl.git/flux_pinn.ipynb:31"
     ]
    }
   ],
   "source": [
    "# double differentiation\n",
    "\n",
    "# f will become our solution for x in [0,1]\n",
    "f=Chain(x->[x],Dense(1, 10, tanh),Dense(10, 10, tanh),Dense(10, 1),first)\n",
    "p=Flux.params(f)\n",
    "\n",
    "# create samples for x in [0,1]\n",
    "n=10\n",
    "x_samples=Flux.rand32(n)\n",
    "\n",
    "# testing the function f\n",
    "# y_samples=f.(x_samples)\n",
    "# y_samples\n",
    "\n",
    "# define the function f_x(x)=df/dx\n",
    "f_x(x)=gradient(f,x)[1]\n",
    "\n",
    "# define our ODE df/dx=-x\n",
    "equation(x)=f_x(x)+1f0*x\n",
    "\n",
    "# Loss is L2 norm of residual (on samples)\n",
    "function loss(p)\n",
    "    residual=equation.(x_samples)\n",
    "    return mean(residual.^2)\n",
    "end\n",
    "\n",
    "val=loss(p)\n",
    "val\n",
    "\n",
    "# try gradient of loss\n",
    "(val,grad)=Flux.withgradient(loss,p) # QUESTION: Zygote does not like loss function?\n",
    "val,grad\n",
    "\n",
    "#CONCLUSION: I still do not know how to use Flux to implement a PINN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching loss()\n\nClosest candidates are:\n  loss(!Matched::Any)\n   @ Main ~/src_nobackup/julia_ml_tests.jl.git/flux_pinn.ipynb:29\n",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching loss()\n",
      "\n",
      "Closest candidates are:\n",
      "  loss(!Matched::Any)\n",
      "   @ Main ~/src_nobackup/julia_ml_tests.jl.git/flux_pinn.ipynb:29\n",
      "\n",
      "\n",
      "Stacktrace:\n",
      " [1] macro expansion\n",
      "   @ ~/.julia/packages/Zygote/JeHtr/src/compiler/interface2.jl:101 [inlined]\n",
      " [2] _pullback(::Zygote.Context{true}, ::typeof(loss))\n",
      "   @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface2.jl:101\n",
      " [3] pullback(f::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "   @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface.jl:384\n",
      " [4] gradient(f::Function, args::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}})\n",
      "   @ Zygote ~/.julia/packages/Zygote/JeHtr/src/compiler/interface.jl:96\n",
      " [5] top-level scope\n",
      "   @ ~/src_nobackup/julia_ml_tests.jl.git/flux_pinn.ipynb:38"
     ]
    }
   ],
   "source": [
    "# double differentiation\n",
    "\n",
    "# f will become our solution for x in [0,1]\n",
    "f=Chain(Dense(1, 10, tanh),Dense(10, 10, tanh),Dense(10, 1))\n",
    "p=Flux.params(f)\n",
    "\n",
    "# create samples for x in [0,1]\n",
    "n=10\n",
    "x_samples=Flux.rand32(1,n) # Flux wants the samples as a final dimension\n",
    "\n",
    "# testing the function f\n",
    "y_samples=f(x_samples)\n",
    "y_samples\n",
    "\n",
    "# define the function f_x(x)=df/dx\n",
    "# The sum is a trick to compute the scalar derivatives of a vector function\n",
    "#dy = gradient(x->sum(f(x)), x_samples)\n",
    "#dy\n",
    "f_x(x)=first(gradient(x->sum(f(x)),x))\n",
    "# dy=f_x(x_samples)\n",
    "# dy\n",
    "\n",
    "# # define our ODE df/dx=-x\n",
    "equation(x)=f_x(x).+1f0.*x\n",
    "# res=equation(x_samples)\n",
    "# res\n",
    "\n",
    "# Loss is L2 norm of residual (on samples)\n",
    "function loss(p)\n",
    "    residuals=equation(x_samples)\n",
    "    return sum(residuals.^2) # QUESTION: abs2 and Flux.mse give errors\n",
    "end\n",
    "\n",
    "# val=loss(p)\n",
    "# val\n",
    "\n",
    "# # try gradient of loss\n",
    "grad=Flux.gradient(loss,p) # QUESTION: Flux.gradient gives error\n",
    "val,grad\n",
    "\n",
    "#CONCLUSION: I still do not know how to use Flux to implement a PINN. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I also asked ChatGPT. It made the same mistakes, but then gave a good suggestion, ie to use the SUM.\n",
    "# This is a nice trick to work with all values at once, instead of one by one.\n",
    "\n",
    "function loss(x)\n",
    "    # Predicted function values\n",
    "    f̂ = model(x)\n",
    "    \n",
    "    # Derivative of the predicted function (gradient wrt x)\n",
    "    df̂ = gradient(x_ -> sum(model(x_)), x)[1]\n",
    "    \n",
    "    # Enforcing f'(x) + x = 0\n",
    "    eq_loss = mean((df̂ .+ x).^2)\n",
    "    \n",
    "    return eq_loss\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
